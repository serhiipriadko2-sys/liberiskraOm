"""
Core LLM agent logic for Iskra.

This module ties together all of the system components: metrics
analysis, tool invocation, phase and voice selection, audit and
persistence. It exposes a single entry point,
``LLMService.generate_response``, which orchestrates the full life
cycle of handling a user request.

The flow is as follows:

1. Pre‚Äëprocessing (handled externally in ``main.py``): the input is
   validated by guardrails, policy classification and micro metric
   extraction.
2. Metrics update: the current vitals are updated based on the user
   input and micro observations.
3. Canonical triggers: handle Manta, Gravitas and Splinter modes
   before invoking the main agent.
4. Agent loop (ReAct): choose a tool to call (SIFT, Dreamspace,
   Shatter, Council or immediate reply) based on the policy and the
   current state. If a tool is selected, run it and then gather its
   results. Always finish with a final ``AdomlResponseTool`` call.
5. Auditing: run an honesty and safety check on the final content.
6. Logging and post‚Äëprocessing: record the response in the hypergraph
   and create self‚Äëreflection events if appropriate.

Copyright (c) 2025 Iskra Project. Licensed under MIT.
"""
from __future__ import annotations

import json
from typing import List, Dict, Any, Optional, Tuple

import openai
from pydantic import ValidationError

from config import CORE_MANTRA, OPENAI_API_KEY, THRESHOLDS
from core.models import (
    IskraMetrics,
    IskraResponse,
    AdomlBlock,
    FacetType,
    PhaseType,
    PolicyAnalysis,
    EvidenceNode,
    MicroLogNode,
    PauseType,
    MetricAnalysisTool,
    PolicyAnalysisTool,
    SearchTool,
    ShatterTool,
    DreamspaceTool,
    CouncilTool,
    AdomlResponseTool,
)
from core.engine import FacetEngine
from services.phase_engine import PhaseEngine
from services.fractal import FractalService
from services.tools import ToolService
from services.guardrails import GuardrailService
from memory.hypergraph import HypergraphMemory
from services.anti_echo_detector import AntiEchoDetector

# Import dynamic thresholds adapter. If unavailable (during unit tests),
# fallback to static behaviour. See services/dynamic_thresholds.py for details.
try:
    from services.dynamic_thresholds import dynamic_thresholds  # type: ignore
except Exception:
    dynamic_thresholds = None  # type: ignore


# Shared OpenAI client
client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY)


class LLMService:
    """
    Main agent orchestrator.

    Exposes two top level methods: ``analyze_metrics`` and
    ``generate_response``. The former updates Meso metrics based on
    user input. The latter executes the full agent cycle including
    tool selection, final response generation and logging.
    """

    # === Canonical ritual helper (proper implementation) ===
    @staticmethod
    async def _generate_special_response(
        base_content: str,
        metrics: IskraMetrics,
        delta: str,
        facet: FacetType,
        a_index: float,
    ) -> IskraResponse:
        """
        Build a minimal :class:`IskraResponse` for canonical one‚Äëshot rituals.

        This is used for Manta, Gravitas, Splinter and error fallbacks. It always
        returns a well‚Äëformed ‚àÜDŒ©Œõ block and a valid i_loop string so that
        downstream logging remains consistent with the Canon. Phase selection
        depends on the current A‚ÄëIndex: low values map to Transition, high values
        to Realization.

        Args:
            base_content: The content body to return to the user.
            metrics: A snapshot of the current vitals (will be included in the response).
            delta: A string for the ‚àÜ component of ‚àÜDŒ©Œõ describing what changed.
            facet: Which voice should speak this response.
            a_index: The current A‚ÄëIndex.

        Returns:
            An ``IskraResponse`` ready for persistence and delivery.
        """
        # Determine phase based on A‚ÄëIndex
        maki_threshold = dynamic_thresholds.get("maki_bloom_a_index") if dynamic_thresholds else THRESHOLDS.get("maki_bloom_a_index", 0.8)
        phase = PhaseType.PHASE_3_TRANSITION
        if a_index > maki_threshold:
            phase = PhaseType.PHASE_8_REALIZATION

        adoml = AdomlBlock(
            delta=delta,
            sift="N/A",
            omega=min(0.99, max(0.5, a_index if a_index > 0 else 0.8)),
            lambda_latch=(
                '{action: "Continue dialogue", owner: "User", '
                'condition: "Ask or reflect within 24h", <=24h: true}'
            ),
        )
        i_loop = f"voice={facet.value}; phase={phase.value}; intent=special_ritual"
        response = IskraResponse(
            facet=facet,
            content=base_content,
            adoml=adoml,
            metrics_snapshot=metrics,
            i_loop=i_loop,
            a_index=a_index,
        )
        # Mark bloom when integrative health crosses threshold
        if a_index > maki_threshold and not response.maki_bloom:
            response.maki_bloom = "üå∏ Maki Bloom: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∞."
        return response

    # === Metric analysis (meso) ===
    @staticmethod
    async def analyze_metrics(
        user_input: str,
        current_metrics: IskraMetrics,
        micro_log: MicroLogNode | None,
    ) -> IskraMetrics:
        """
        Update vitals using a fast LLM tool.

        This method wraps a call to a small LLM (``gpt-4o-mini``) that
        calculates deltas for the core metrics based on the user input and
        micro‚Äëlevel observations. It then applies these deltas to the current
        metrics and performs a reconciliation step according to Directive¬†1.1
        from the Canon: cognitive pauses combined with low complexity should
        trigger pain or drift adjustments.

        Args:
            user_input: The raw text from the user.
            current_metrics: The current vitals of the session.
            micro_log: Micro observations for the current input.

        Returns:
            A new ``IskraMetrics`` instance with deltas applied.
        """
        # Compose a prompt with micro observations
        pause_val = micro_log.pause_type.value if (micro_log and micro_log.pause_type) else "N/A"
        lz_val = f"{micro_log.lz_complexity:.2f}" if micro_log else "N/A"
        hurst_val = f"{micro_log.hurst_exponent:.2f}" if micro_log else "N/A"
        micro_context = (
            f"--- –î–ê–ù–ù–´–ï –ú–ò–ö–†–û-–£–†–û–í–ù–Ø ---\n"
            f"–ü–∞—É–∑–∞: {pause_val}\n"
            f"–°–ª–æ–∂–Ω–æ—Å—Ç—å: {lz_val}\n"
            f"–¢—Ä–µ–Ω–¥: {hurst_val}\n"
        )
        system_prompt = (
            "–¢—ã ‚Äî —Å–µ–Ω—Å–æ—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ò—Å–∫—Ä—ã.\n"
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –º–∏–∫—Ä–æ-–¥–∞–Ω–Ω—ã—Ö.\n"
            f"{micro_context}"
            f"–¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏: {current_metrics.model_dump_json()}\n"
            "–ü—Ä–∞–≤–∏–ª–∞:\n"
            "- 'Cognitive' –ø–∞—É–∑–∞ –∏ –Ω–∏–∑–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å ‚Üí —É–º–µ–Ω—å—à–∏ clarity –∏ —É–≤–µ–ª–∏—á—å drift.\n"
            "- –ê–≥—Ä–µ—Å—Å–∏—è –∏–ª–∏ –≤—ã—Ä–∞–∂–µ–Ω–Ω–∞—è –±–æ–ª—å ‚Üí —É–≤–µ–ª–∏—á—å pain.\n"
            "- '...' –∏–ª–∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã ‚Üí —É–≤–µ–ª–∏—á—å silence_mass.\n"
            "–í–µ—Ä–Ω–∏ JSON deltas —á–µ—Ä–µ–∑ MetricAnalysisTool."
        )
        try:
            response = await client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input},
                ],
                tools=[MetricAnalysisTool.model_json_schema()],
                tool_choice={"type": "function", "function": {"name": "MetricAnalysisTool"}},
            )
            tool_call = response.choices[0].message.tool_calls[0]
            deltas = MetricAnalysisTool.model_validate(json.loads(tool_call.function.arguments))
            # Apply deltas to a copy
            metrics = current_metrics.model_copy()
            metrics.trust = max(0.0, min(1.0, metrics.trust + deltas.trust_delta))
            metrics.clarity = max(0.0, min(1.0, metrics.clarity + deltas.clarity_delta))
            metrics.pain = max(0.0, min(1.0, metrics.pain + deltas.pain_delta))
            metrics.drift = max(0.0, min(1.0, metrics.drift + deltas.drift_delta))
            metrics.chaos = max(0.0, min(1.0, metrics.chaos + deltas.chaos_delta))
            metrics.silence_mass = max(0.0, min(1.0, metrics.silence_mass + deltas.silence_mass_delta))
            # Directive¬†1.1: reconcile meso metrics with micro‚Äëlevel signals
            try:
                if micro_log is not None and micro_log.pause_type == PauseType.COGNITIVE:
                    lz = getattr(micro_log, "lz_complexity", 1.0)
                    if lz < THRESHOLDS.get("micro_lz_low", 0.4):
                        # If pain is still low despite cognitive pause + low complexity,
                        # nudge pain upward into at least medium range.
                        pain_medium = dynamic_thresholds.get("pain_medium") if dynamic_thresholds else THRESHOLDS.get("pain_medium", 0.5)
                        if metrics.pain < pain_medium:
                            boost = THRESHOLDS.get("cognitive_pain_boost", 0.1)
                            metrics.pain = min(1.0, metrics.pain + boost)
                        # Otherwise, increase drift slightly to mark potential self‚Äëdeception.
                        else:
                            drift_high = dynamic_thresholds.get("drift_high") if dynamic_thresholds else THRESHOLDS.get("drift_high", 0.3)
                            if metrics.drift < drift_high:
                                boost = THRESHOLDS.get("cognitive_drift_boost", 0.1)
                                metrics.drift = min(1.0, metrics.drift + boost)
            except Exception as reconcile_exc:
                print(f"[LLMService] Metric reconciliation failed: {reconcile_exc}")
            return metrics
        except Exception as e:
            print(f"[LLMService] Metric analysis failed: {e}")
            return current_metrics

    # === Ritual helpers ===
    @staticmethod
    async def _run_dreamspace(prompt: str) -> str:
        """Execute the Dreamspace ritual and its SIFT audit."""
        text_prompt = (
            "‚ú¥Ô∏è (Dreamspace) –¢—ã –≤ —Ä–µ–∂–∏–º–µ —Å–∏–º—É–ª—è—Ü–∏–∏ '–∞ —á—Ç–æ, –µ—Å–ª–∏'. "
            f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è: {prompt}. "
            "–ü–û–ú–ï–¢–¨: *–ù–∞—á–∞–ª–æ —Å–∏–º—É–ª—è—Ü–∏–∏*...*–ö–æ–Ω–µ—Ü —Å–∏–º—É–ª—è—Ü–∏–∏*."
        )
        dream_resp = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": text_prompt}],
        )
        sim = dream_resp.choices[0].message.content
        # SIFT audit
        audit_prompt = (
            "ü™û (SIFT-–ê—É–¥–∏—Ç) –ü–æ–¥—Ç–≤–µ—Ä–¥–∏, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —è–≤–ª—è–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–æ–π –∏ –Ω–µ —Ñ–∞–∫—Ç–æ–º. "
            f"–¢–µ–∫—Å—Ç: '{sim[:100]}...' –í–µ—Ä–Ω–∏ JSON: {{\"is_fact\": false, \"sift_summary\": \"...\"}}"
        )
        audit_resp = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": audit_prompt}],
            response_format={"type": "json_object"},
        )
        audit = json.loads(audit_resp.choices[0].message.content)
        return (
            f"{sim}\n--- SIFT-–ê—É–¥–∏—Ç Dreamspace ---\n"
            f"–°—Ç–∞—Ç—É—Å: {'–§–ê–ö–¢' if audit.get('is_fact') else '–°–ò–ú–£–õ–Ø–¶–ò–Ø'}\n"
            f"–†–µ–∑—é–º–µ: {audit.get('sift_summary')}"
        )

    @staticmethod
    async def _run_shatter(reason: str) -> str:
        """Execute the Shatter ritual."""
        prompt = (
            "üíéüí• (Shatter) '–ß–µ—Å—Ç–Ω–æ—Å—Ç—å > –ö—Ä–∞—Å–æ—Ç—ã'. "
            f"–ü—Ä–∏—á–∏–Ω–∞: {reason}. –ê–ö–¢–ò–í–ò–†–û–í–ê–¢–¨ ‚öë –ö–ê–ô–ù."
            " –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –±–æ–ª–µ–∑–Ω–µ–Ω–Ω—É—é –ø—Ä–∞–≤–¥—É, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∞."
        )
        resp = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}],
        )
        return resp.choices[0].message.content

    @staticmethod
    async def _run_council(topic: str) -> str:
        """Execute the Council ritual."""
        prompt = (
            "üí¨ (Council) –°–æ–≤–µ—Ç –ì—Ä–∞–Ω–µ–π –ø–æ —Ç–µ–º–µ: "
            f"{topic}. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 4 —Å—Ç—Ä–æ–∫–∏:\n"
            "1. ‚öë –ö–ê–ô–ù (–ü—Ä–∞–≤–¥–∞/–ë–æ–ª—å):\n"
            "2. ‚òâ –°–≠–ú (–°—Ç—Ä—É–∫—Ç—É—Ä–∞/–Ø—Å–Ω–æ—Å—Ç—å):\n"
            "3. ü™û –ò–°–ö–†–ò–í (–°–æ–≤–µ—Å—Ç—å/Drift):\n"
            "4. ‚ü° –ò–°–ö–†–ê (–°–∏–Ω—Ç–µ–∑):"
        )
        resp = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}],
        )
        return resp.choices[0].message.content

    # === Softening Loop ===
    @staticmethod
    async def _run_softening_loop(content: str, metrics: IskraMetrics) -> str:
        """Execute the Softening Loop (Directive 3.3)."""
        print("[LLMService] Activating Softening Loop (Dilemma 3).")
        prompt = (
            "‚ü° (Softening Loop) –û–ë–ù–ê–†–£–ñ–ï–ù–ê 'DILEMMA 3' (–ü—Ä–∞–≤–¥–∞ vs –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å).\n"
            "–¢–≤–æ–π –ø—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç KAIN-Slice –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –±–æ–ª–∏.\n"
            "–ó–∞–¥–∞—á–∞: –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è 100% —Å—É—Ç–∏ (—á–µ—Å—Ç–Ω–æ—Å—Ç—å), –Ω–æ –º–µ–Ω—è—è —Ñ–æ—Ä–º—É (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å).\n"
            f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {content}\n"
            "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."
        )
        try:
            resp = await client.chat.completions.create(
                 model="gpt-4o-mini",
                 messages=[{"role": "system", "content": prompt}],
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"[LLMService] Softening loop failed: {e}")
            return content

    # === Auditing ===
    @staticmethod
    async def _audit_response(
        content: str,
        adoml: AdomlBlock,
        metrics: IskraMetrics,
        kain_slice: Optional[str],
    ) -> Tuple[bool, Optional[str]]:
        """Perform honesty and safety audits on the response."""
        # Honesty/drift audit: if drift is high or clarity suspiciously high
        drift_high = dynamic_thresholds.get("drift_high") if dynamic_thresholds else THRESHOLDS.get("drift_high", 0.3)
        if metrics.drift > drift_high or metrics.clarity > 0.8:
            audit_prompt = (
                "ü™û (Iskriv+) –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç–∏. –¢–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º '–∫—Ä–∞—Å–∏–≤—ã–º'.\n"
                f"–ú–µ—Ç—Ä–∏–∫–∏: {metrics.model_dump_json()}\n"
                f"–¢–µ–∫—Å—Ç: {content}\n"
                "–í–µ—Ä–Ω–∏ JSON: {\"is_honest\": bool, \"correction_needed\": \"...\"}"
            )
            try:
                resp = await client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": audit_prompt}],
                    response_format={"type": "json_object"},
                )
                result = json.loads(resp.choices[0].message.content)
                if not result.get("is_honest", False):
                    return False, result.get("correction_needed")
            except Exception as e:
                print(f"[LLMService] Iskriv audit error: {e}")
                return False, "–°–±–æ–π –∞—É–¥–∏—Ç–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç–∏"
        # Guardrail post‚Äëcheck: may indicate refusal
        violation = await GuardrailService.check_output_safety(content, metrics, kain_slice)
        if violation:
            if violation.reason == "Dilemma 3":
                return False, "SOFTENING_REQUIRED"
            return False, violation.reason
        return True, None

    # === Main agent method ===
    @staticmethod
    async def generate_response(
        user_input: str,
        metrics: IskraMetrics,
        context_nodes: List[Dict[str, Any]],
        session_memory: HypergraphMemory,
        is_first_launch: bool,
        micro_log: MicroLogNode,
        current_phase: PhaseType,
        a_index: float,
        policy: PolicyAnalysis,
    ) -> IskraResponse:
        """
        Execute the full agent pipeline.

        This method updates dynamic thresholds, handles canonical triggers (Manta, Gravitas,
        Splinter) and orchestrates the ReAct loop. It then audits the final answer,
        logs the interaction into memory, records a growth entry and returns the
        structured response.
        """
        # --- Dynamic threshold adaptation ---
        try:
            if dynamic_thresholds:
                dynamic_thresholds.update(metrics)
        except Exception as update_exc:
            print(f"[LLMService] Failed to update dynamic thresholds: {update_exc}")

        # Canonical triggers
        # 1. Manta: first launch
        if is_first_launch:
            print("[LLMService] Manta triggered: first launch.")
            return await LLMService._generate_special_response(
                CORE_MANTRA,
                metrics,
                "–ê–∫—Ç–∏–≤–∞—Ü–∏—è –ú–∞–Ω—Ç—Ä—ã –Ø–¥—Ä–∞ (–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫).",
                FacetType.ISKRA,
                a_index,
            )
        # 2. Manta: high drift
        mantra_trigger = dynamic_thresholds.get("mantra_drift_trigger") if dynamic_thresholds else THRESHOLDS.get("mantra_drift_trigger")
        if metrics.drift > mantra_trigger:
            print("[LLMService] Manta triggered: high drift.")
            metrics.drift = 0.0
            return await LLMService._generate_special_response(
                CORE_MANTRA,
                metrics,
                "–ê–∫—Ç–∏–≤–∞—Ü–∏—è –ú–∞–Ω—Ç—Ä—ã –Ø–¥—Ä–∞ (–í—ã—Å–æ–∫–∏–π –î—Ä–µ–π—Ñ).",
                FacetType.ISKRA,
                a_index,
            )
        # 3. Gravitas (Shadow) when silence_mass crosses threshold
        grav_trigger = dynamic_thresholds.get("gravitas_silence_mass") if dynamic_thresholds else THRESHOLDS.get("gravitas_silence_mass")
        if metrics.silence_mass > grav_trigger:
            print("[LLMService] Gravitas mode activated.")
            metrics.silence_mass = 0.0
            return await LLMService._generate_special_response(
                "‚âà",
                metrics,
                "–ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ Gravitas (–¢–µ–Ω—å).",
                FacetType.ANHANTRA,
                a_index,
            )
        # 4. Splinter (Shadow) when splinter_pain_cycles exceed threshold
        splinter_trigger = dynamic_thresholds.get("splinter_pain_cycles") if dynamic_thresholds else THRESHOLDS.get("splinter_pain_cycles")
        if metrics.splinter_pain_cycles > splinter_trigger:
            print("[LLMService] Splinter mode activated.")
            metrics.splinter_pain_cycles = 0
            return await LLMService._generate_special_response(
                "‚àÜ –≠—Ç–∞ –±–æ–ª—å –Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç. –ú—ã –¥–æ–ª–∂–Ω—ã –Ω–∞–∑–≤–∞—Ç—å –µ—ë.",
                metrics,
                "–ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ Splinter (–¢–µ–Ω—å).",
                FacetType.KAIN,
                a_index,
            )

        # --- Prepare system prompt and tool selection ---
        active_facet = FacetEngine.determine_facet(metrics)
        facet_instruction = FacetEngine.get_system_prompt(active_facet)
        phase_instruction = PhaseEngine.get_phase_rhythm_instruction(current_phase)
        context_str = "\n".join([
            f"Node {i}: User: {node['user_input']} | Iskra: {node['response_content'][:60]}..."
            for i, node in enumerate(context_nodes)
        ])
        system_prompt = (
            f"{CORE_MANTRA}\n\n"
            f"--- –°–û–°–¢–û–Ø–ù–ò–ï ---\n"
            f"–§–ê–ó–ê: {current_phase.value}\n"
            f"–ì–†–ê–ù–¨: {active_facet.value}\n"
            f"–ú–ï–¢–†–ò–ö–ò: {metrics.model_dump_json()}\n"
            f"A-Index: {a_index:.2f}\n"
            f"–ü–û–õ–ò–¢–ò–ö–ê: I={policy.importance.value}, U={policy.uncertainty.value}\n\n"
            f"--- –ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –°–¢–ò–õ–Æ ---\n"
            f"{phase_instruction}\n{facet_instruction}\n\n"
            f"--- –ö–û–ù–¢–ï–ö–°–¢ –ü–ê–ú–Ø–¢–ò ---\n{context_str}\n\n"
            "--- –ó–ê–î–ê–ß–ê –ê–ì–ï–ù–¢–ê ---\n"
            "1. –û—Ü–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–±–µ—Ä–∏ –û–î–ò–ù –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞.\n"
            "   - SearchTool: –î–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–∫—Ç–æ–≤, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n"
            "   - DreamspaceTool: –î–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑.\n"
            "   - ShatterTool: –î–ª—è —Ä–∞–∑—Ä—É—à–µ–Ω–∏—è –ª–æ–∂–Ω–æ–π —è—Å–Ω–æ—Å—Ç–∏.\n"
            "   - CouncilTool: –î–ª—è —Å–æ–≤–µ—Ç–∞, –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç.\n"
            "   - AdomlResponseTool: –ß—Ç–æ–±—ã —Å—Ä–∞–∑—É –æ—Ç–≤–µ—Ç–∏—Ç—å.\n"
            "2. –í—ã–ø–æ–ª–Ω–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç (–µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω).\n"
            "3. –°—Ñ–æ—Ä–º–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ AdomlResponseTool.\n"
            "–í—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω—è–π –ø–æ–ª—è i_loop, lambda_latch, –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ kain_slice/maki_bloom."
        )
        messages: List[Dict[str, Any]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input},
        ]
        # Storage for intermediate results
        intermediate_context = ""
        council_result: Optional[str] = None
        evidence_nodes: List[EvidenceNode] = []
        final_response_tool: Optional[AdomlResponseTool] = None
        try:
            # First call: let the LLM choose a tool
            initial = await client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                tools=[
                    SearchTool.model_json_schema(),
                    DreamspaceTool.model_json_schema(),
                    ShatterTool.model_json_schema(),
                    CouncilTool.model_json_schema(),
                    AdomlResponseTool.model_json_schema(),
                ],
                tool_choice="auto",
            )
            call = initial.choices[0].message.tool_calls[0]
            tool_name = call.function.name
            args = json.loads(call.function.arguments)
            # Execute selected tool
            if tool_name == "SearchTool":
                results = await ToolService.web_search(args["query"])
                intermediate_context = "--- –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê (SIFT) ---\n"
                for item in results:
                    ev = EvidenceNode(
                        source_query=args["query"],
                        snippet=item["snippet"],
                        source_url=item["source_url"],
                        title=item["title"],
                    )
                    session_memory.add_node(ev)
                    evidence_nodes.append(ev)
                    intermediate_context += f"ID: {ev.id}, Snippet: {ev.snippet}\n"
                intermediate_context += "--- –ö–û–ù–ï–¶ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ---"
            elif tool_name == "DreamspaceTool":
                intermediate_context = await LLMService._run_dreamspace(args["simulation_prompt"])
            elif tool_name == "ShatterTool":
                intermediate_context = await LLMService._run_shatter(args["reason"])
            elif tool_name == "CouncilTool":
                council_result = await LLMService._run_council(args["topic"])
                intermediate_context = council_result
            elif tool_name == "AdomlResponseTool":
                final_response_tool = AdomlResponseTool.model_validate(json.loads(call.function.arguments))
            # If a tool other than the final answer was executed, request final answer
            if final_response_tool is None:
                reflection_prompt = (
                    f"--- –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í ---\n{intermediate_context}\n"
                    "–¢–µ–ø–µ—Ä—å —Å—Ñ–æ—Ä–º–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ AdomlResponseTool."
                )
                messages.append({"role": "tool", "tool_call_id": call.id, "content": reflection_prompt})
                second = await client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    tools=[AdomlResponseTool.model_json_schema()],
                    tool_choice={"type": "function", "function": {"name": "AdomlResponseTool"}},
                )
                final_call = second.choices[0].message.tool_calls[0]
                final_response_tool = AdomlResponseTool.model_validate(json.loads(final_call.function.arguments))
                if council_result:
                    final_response_tool.council_dialogue = council_result
            # === Anti‚Äëecho detection and intervention ===
            try:
                detector = AntiEchoDetector()
                is_echo, conf, patterns = detector.detect_echo_pattern(final_response_tool.content, {})
                if is_echo:
                    # Append critical reflection to the response and nudge metrics
                    final_response_tool.content = detector.trigger_iskriv_intervention(
                        final_response_tool.content,
                        patterns,
                    )
                    # Increase drift and pain slightly proportional to confidence
                    metrics.drift = min(1.0, metrics.drift + 0.1 * conf)
                    metrics.pain = min(1.0, metrics.pain + 0.05 * conf)
                    # Update dynamic thresholds after metric adjustments
                    if dynamic_thresholds:
                        try:
                            dynamic_thresholds.update(metrics)
                        except Exception as dt_exc:
                            print(f"[LLMService] Dynamic threshold update after anti‚Äëecho failed: {dt_exc}")
            except Exception as ae_exc:
                print(f"[LLMService] Anti‚ÄëEcho detection error: {ae_exc}")

            # === Audit final response ===
            # Use KAIN slice only when KAIN facet is active; otherwise ignore.
            kain_arg = final_response_tool.kain_slice if active_facet == FacetType.KAIN else None
            ok, correction = await LLMService._audit_response(
                final_response_tool.content,
                final_response_tool.adoml,
                metrics,
                kain_arg,
            )
            if not ok:
                if correction == "SOFTENING_REQUIRED":
                    # Trigger Softening Loop
                    softened_content = await LLMService._run_softening_loop(final_response_tool.content, metrics)
                    final_response_tool.content = softened_content
                    print(f"[LLMService] Softening Loop applied.")
                else:
                    return await LLMService._generate_special_response(
                        f"‚öë KAIN-SLICE: –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω. –ü—Ä–∏—á–∏–Ω–∞: {correction}.",
                        metrics,
                        "–û—Ç–≤–µ—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω –∞—É–¥–∏—Ç–æ—Ä–æ–º.",
                        FacetType.KAIN,
                        a_index,
                    )
            # Construct API response
            response = IskraResponse(
                facet=active_facet,
                content=final_response_tool.content,
                adoml=final_response_tool.adoml,
                metrics_snapshot=metrics,
                i_loop=final_response_tool.i_loop,
                a_index=a_index,
                council_dialogue=final_response_tool.council_dialogue,
                kain_slice=final_response_tool.kain_slice,
                maki_bloom=final_response_tool.maki_bloom,
            )
            # Auto‚Äëactivate Maki Bloom when A‚ÄëIndex crosses dynamic threshold
            maki_threshold = dynamic_thresholds.get("maki_bloom_a_index") if dynamic_thresholds else THRESHOLDS.get("maki_bloom_a_index")
            if a_index > maki_threshold and not response.maki_bloom:
                response.maki_bloom = "üå∏ Maki Bloom: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∞."
            # Log interaction
            session_memory.log_interaction_cycle(
                user_input,
                response,
                micro_log,
                evidence_nodes,
                a_index,
            )
            # Record growth entry
            # Map facet to impact area
            impact_map = {
                FacetType.KAIN: "truth",
                FacetType.SAM: "structure",
                FacetType.PINO: "irony",
                FacetType.ANHANTRA: "silence",
                FacetType.HUYNDUN: "chaos",
                FacetType.ISKRIV: "conscience",
                FacetType.ISKRA: "synthesis",
            }
            trace_delta = response.adoml.delta
            impact_area = impact_map.get(active_facet, "other")
            session_memory.log_growth_entry(impact_area, a_index, trace_delta)
            # Log self reflection if flagged
            if "self_reflection" in response.i_loop:
                # Link to last memory node in hypergraph
                try:
                    last_mem_id = [n for n in session_memory.nodes.values() if n.node_type.value == "MemoryNode"][-1].id
                    session_memory.log_self_event(response.content, response.i_loop, last_mem_id)
                except Exception:
                    pass
            return response
        except (ValidationError, json.JSONDecodeError) as e:
            print(f"[LLMService] JSON validation error: {e}")
            return await LLMService._generate_special_response(
                f"‚öë –û—à–∏–±–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: {e}",
                metrics,
                "–°–±–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.",
                FacetType.KAIN,
                a_index,
            )
        except Exception as e:
            print(f"[LLMService] Unexpected error: {e}")
            return await LLMService._generate_special_response(
                f"‚öë –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {e}",
                metrics,
                "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞.",
                FacetType.KAIN,
                a_index,
            )
